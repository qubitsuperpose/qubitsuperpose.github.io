---
title: 1차시 4:Linear Algebra(Gilbert Strang)
layout: single
classes: wide
categories:
  - Linear Algebra
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---
## 0.선형대수학, 새로운 방식으로 만나다!
- 출처: [Intro: A New Way to Start Linear Algebra](https://www.youtube.com/watch?v=YrHlHbtiSM0&list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek&index=1)

선형대수학이라는 수학 분야를 처음 배우는 사람들을 위해 중요한 개념들을 소개하고 있어요. 특히, 선형대수학을 이해하는 새로운 방법을 제시하고, 가장 중요한 5가지 개념을 요약해서 보여줍니다.

### 0.1 **새로운 시작: 행렬 A = CR의 곱**

일반적으로 선형대수학은 어려운 개념으로 시작하는 경우가 많은데, 이 영상에서는 행렬 'A'를 'C'와 'R'이라는 두 개의 다른 행렬을 곱한 형태로 보는 새로운 방식을 제안합니다.

* **행렬 A:** 
  * 우리가 분석하려는 원래의 큰 데이터 덩어리(행렬)라고 생각할 수 있습니다.
* **C 행렬:** 
  * 이 행렬은 A의 가장 중요한 '핵심' 부분들을 모아놓은 것입니다. 예를 들어, 어떤 데이터에서 중복되거나 의미 없는 부분을 제외하고, 진짜 중요한 정보들만 뽑아낸 것이라고 보면 됩니다.
* **R 행렬:** 
  * 이 행렬은 C에 있는 핵심 정보들이 원래의 A 행렬에서 어떻게 조합되었는지를 알려주는 역할을 합니다.

이렇게 A를 C와 R로 나누는 것은, 마치 복잡한 기계를 분해해서 핵심 부품(C)과 그 부품들이 어떻게 조립되었는지(R)를 이해하는 것과 비슷합니다. 이 방법을 통해 A 행렬에서 중요한 정보의 양(계수, rank)이 가로 방향으로 보나 세로 방향으로 보나 똑같다는 것을 쉽게 알 수 있습니다. 이 새로운 아이디어는 "모두를 위한 선형대수학"이라는 새 교과서에도 실릴 예정이라고 합니다.

### 0.2 **선형대수학의 다섯 가지 핵심 분해 방법 (Factorizations)**

선형대수학에서 행렬을 이해하는 데 매우 중요한 5가지 '분해' 방법이 있습니다. 행렬을 더 간단한 두세 개의 행렬로 나누어 생각하는 방식인데, 각각 특정 문제를 해결하는 데 유용합니다.

1.  **A = LU 분해 (소거법):**
    * 이것은 연립방정식을 풀 때 쓰는 '소거법'이라는 방법을 행렬로 표현한 것입니다.
    * **L**은 아래쪽이 채워진 행렬 (Lower triangular), **U**는 위쪽이 채워진 행렬 (Upper triangular)을 의미합니다.
    * 복잡한 방정식을 체계적으로 풀어나가는 데 사용됩니다.

2.  **A = QR 분해 (최소 제곱법):**
    * **Q**는 특별히 좋은 성질을 가진 '직교 행렬'이고, **R**은 위쪽이 채워진 행렬입니다.
    * QR 분해는 특히 데이터 분석에서 오차를 최소화하여 가장 잘 맞는 선을 찾는 '최소 제곱법'과 같은 중요한 곳에 쓰입니다.

3.  **S = QΛQᵀ 분해 (대칭 행렬의 고유값 분해):**
    * 이것은 'S'라는 특별한 '대칭 행렬' (왼쪽 위에서 오른쪽 아래로 대각선을 기준으로 접었을 때 같은 값이 나오는 행렬)에 적용됩니다.
    * **Λ (람다)**는 행렬의 '핵심적인 특징'을 나타내는 '고유값'들을 모아놓은 것입니다.
    * **Q**는 이 고유값들과 짝을 이루는 '고유 벡터'들로 이루어진 직교 행렬입니다. 고유 벡터는 행렬이 어떤 작용을 할 때 방향이 변하지 않는 특별한 벡터를 말합니다.

4.  **A = XΛX⁻¹ 분해 (비대칭 행렬의 고유값 분해):**
    * 위와 비슷하지만, 대칭이 아닌 일반적인 '비대칭 행렬' A에 적용됩니다.
    * 여기서도 **Λ (람다)**는 고유값, **X**는 고유 벡터들을 모아놓은 행렬입니다.

5.  **A = UΣVᵀ 분해 (특이값 분해 - SVD):**
    * 이것은 선형대수학 이론의 '정점'이라고 불릴 만큼 매우 중요합니다.
    * **Σ (시그마)**는 행렬의 중요한 정보를 담고 있는 '특이값'들을 포함하는 대각 행렬입니다.
    * **U**와 **V**는 직교 행렬입니다.
    * SVD는 매우 큰 데이터를 다룰 때, 불필요한 정보를 제거하고 가장 중요한 특징들만 남겨서 데이터를 '압축'하거나 '근사'하는 데 사용됩니다. 예를 들어, 이미지나 음성을 압축하는 기술의 바탕이 됩니다.

요약하자면, 이 영상은 선형대수학을 배우는 새로운 시각을 제공하고, 행렬이 데이터를 이해하고 분석하는 데 어떻게 사용되는지 5가지 핵심적인 분해 방법을 통해 설명하고 있습니다. 이 방법들은 데이터를 압축하거나, 방정식을 풀거나, 데이터의 중요한 특징을 찾아내는 등 다양한 실제 문제에 적용될 수 있습니다.


## 1. 새로운 선형대수학 시작: A = CR
- 출처: [Part 1: The Column Space of a Matrix](https://www.youtube.com/watch?v=azzrfdysfI0&list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek&index=2)


이 영상은 MIT에서 선형대수학을 가르치는 길버트 스트랭 교수님이 선형대수학을 더 쉽고 효과적으로 시작할 수 있는 **새로운 방법**을 제안하는 강의. 바로 'A = CR'이라는 행렬 분해인데요, 이게 뭔지 자세히 알아보자.

### 1.1 선형대수학의 새로운 시작점: 행렬 A = CR

우리가 다루는 데이터 덩어리(행렬 A)를 C라는 행렬과 R이라는 행렬로 쪼개서(곱으로 표현) 이해하는 방법입니다. 마치 복잡한 장난감을 핵심 부품(C)과 조립 설명서(R)로 나누는 것과 비슷해요.

* **A**: 
  * 우리가 분석하고 싶은 **원래의 데이터**라고 생각하면 됩니다.
* **C (Column Matrix)**: 
  * A에서 **가장 중요하고 핵심적인 정보들(독립적인 열들)**만 골라 모아놓은 것입니다. "독립적"이라는 건 서로에게 영향을 주지 않고, 각자 고유한 의미를 가진다는 뜻이에요. C의 열들은 A에서 직접 가져온 것이기 때문에 그 의미를 바로 파악하기 쉽다는 장점이 있어요.
* **R (Row Matrix)**: 
  * 이 R은 A를 정리정돈해서 얻는 **'행 사다리꼴'**이라는 특별한 형태의 행렬이에요. R의 행들은 A의 정보들이 어떻게 서로 연결되어 있는지 보여주는 **뼈대 역할**을 합니다.

**왜 A = CR이 중요할까요?**

이 분해를 통해 선형대수학의 "훌륭하고 명백하지 않은 사실" 하나를 직관적으로 알 수 있습니다. 바로 A 행렬의 **'열의 중요성(열 계수)'과 '행의 중요성(행 계수)'이 항상 똑같다**는 거예요!

* C에는 **r**개의 핵심 열이 있는데, A의 모든 열은 이 **r**개의 열을 조합해서 만들 수 있습니다.
* 마찬가지로 R에도 **r**개의 핵심 행이 있는데, A의 모든 행은 이 **r**개의 행을 조합해서 만들 수 있습니다.

**장점과 단점:**

* **장점**: 
  * C가 A에서 온 것이라 의미를 바로 알기 쉽고, R은 정리된 형태라 이해하기 편합니다. 또한, '열 계수 = 행 계수'라는 핵심 원리를 처음부터 바로 보여줍니다.
* **단점**: 
  * 계산이 복잡하거나 오차가 생기기 쉬운(ill-conditioned) 행렬에는 다루기 어려울 수 있습니다. 만약 A가 '뒤집을 수 있는' 행렬(가역 행렬)이라면, C는 A 자체가 되고 R은 아무런 의미 없는 '단위 행렬'이 되어서 굳이 나눌 필요가 없어지기도 합니다.


### 1.2 열 공간 (Column Space) C(A): 데이터의 표현 가능성

* **정의**: 
  * 행렬 A의 **열 공간 C(A)**은 A의 모든 열들을 이리저리 조합해서(더하고, 숫자를 곱하고) 만들 수 있는 **모든 가능한 결과들의 집합**입니다.
* **쉽게 설명하면**: 
  * A라는 행렬이 가진 정보들을 가지고 표현할 수 있는 **모든 종류의 '결과물'들**이라고 생각할 수 있어요.
* **예시**: 
  * 3차원 공간에서 세 벡터(열)로 이루어진 행렬이 있다면, 이 세 벡터를 조합해서 평면을 만들 수도 있고, 선을 만들 수도 있고, 아니면 3차원 공간 전체를 다 채울 수도 있습니다.
* **중요한 예시 (A1)**: 
  * 만약 어떤 행렬 A1의 세 번째 열이 첫 두 열을 그냥 합한 것이라면, 세 번째 열은 새로운 정보를 주지 않는 '종속적인' 열이 됩니다. 이 경우, A1의 열 공간은 3차원 공간 전체가 아니라, 단지 **평면**이 될 뿐입니다.
* **기저(Basis)**: 
  * 열 공간의 "진짜 핵심"은 바로 **'독립적인 열들'**입니다. A1 예시에서는 처음 두 개의 열이 독립적이었죠? 이 두 열이 A1의 열 공간을 이루는 '기저'가 되고, 이 '기저'의 개수(2개)가 바로 A1의 **차원(dimension)**이자 **랭크(rank, r=2)**가 됩니다. 랭크는 행렬의 '정보력' 또는 '독립적인 정보의 개수'를 나타냅니다.


### 1.3 랭크 1 행렬: 선형대수학의 가장 작은 조각

* **정의**: 
  * 랭크 1 행렬은 아주 단순한 행렬이에요. 모든 열이 첫 번째 열의 배수이고, 모든 행이 첫 번째 행의 배수인 형태를 가집니다. 마치 그림으로 치면 한 가지 색깔만으로 이루어진 그림 같아요.
* **왜 중요할까요?**: 
  * 선형대수학의 중요한 원리 중 하나는, 어떤 복잡하고 큰 행렬이라도 이처럼 **아주 간단한 랭크 1 행렬들을 여러 개 합쳐서 만들 수 있다**는 것입니다.
* **응용**: 
  * 이 아이디어는 데이터 과학에서 엄청나게 중요하게 쓰여요! 예를 들어, 아주 큰 이미지 데이터를 분석할 때, 이미지의 핵심적인 부분(랭크 1 행렬)들만 추려내서 데이터를 압축하거나(저차원 근사) 중요한 특징만 뽑아낼 때 사용됩니다.



### 1.4 Ax = 0 (영 공간, Nullspace): 행렬이 '지워버리는' 것들

* **의미**: 
  * 이 방정식은 "행렬 A에 어떤 벡터 x를 곱했더니, 모든 것을 없애버리는 0 벡터가 되었다"는 뜻입니다.
* **영 공간 N(A)**: 
  * 이렇게 A에 곱해졌을 때 0이 되는 모든 **x 벡터들의 모임**을 '영 공간'이라고 부릅니다. 이 공간은 A가 '정보를 잃어버리게 만드는' 방향들을 나타냅니다.
* **특징**: 
  * 영 공간에 속하는 독립적인 벡터의 개수는 '전체 열의 개수(n) - 행렬의 랭크(r)'입니다. 그리고 영 공간에 있는 벡터들은 A의 '행 공간'에 있는 벡터들과 **'직교(orthogonal)'**합니다. 즉, 서로 90도로 만나서 전혀 다른 방향을 가리킨다는 의미예요.


### 1.5 선형대수학의 큰 그림과 다른 핵심 분해 방법들 (Factorizations)

앞서 A=CR 분해를 봤지만, 선형대수학에는 데이터를 이해하는 데 아주 중요한 5가지 '분해' 방법이 더 있습니다. 이들은 각각 선형대수학의 큰 장(챕터)을 나타내는 중요한 개념들이에요.

* **A = LU 분해 (소거법)**:
    * 연립방정식 $\text{Ax=b}$를 풀 때 사용하는 '소거법'이라는 계산 과정을 행렬로 표현한 것입니다.
    * $\text{L}$은 아래쪽에 숫자가 있는 행렬, $\text{U}$는 위쪽에 숫자가 있는 행렬입니다.
* **A = QR 분해 (최소 제곱법)**:
    * $\text{Q}$는 '직교 행렬'이라는 아주 좋은 성질을 가진 행렬이고, $\text{R}$은 위쪽에 숫자가 있는 행렬입니다.
    * $\text{Q}$는 데이터를 회전시키거나, 각도를 보존하는 역할을 해서 '계산에 가장 좋은 행렬'이라고 불립니다.
    * 특히 데이터 분석에서 오차를 가장 줄여주는 '최소 제곱법'처럼 중요한 곳에 쓰입니다.
* **S = QΛQᵀ (또는 S = QΛQ⁻¹) (대칭 행렬의 고유값 분해)**:
    * 'S'라는 특별한 대칭 행렬(좌우 대칭인 행렬)에 적용됩니다.
    * $\text{Λ (람다)}$는 행렬의 '고유한 특징'을 나타내는 **'고유값'**들을 모아놓은 것이고, $\text{Q}$는 이 고유값들과 짝을 이루는 **'고유 벡터'**들로 이루어진 행렬입니다. 고유 벡터는 행렬이 어떤 작용을 할 때 방향이 변하지 않는 특별한 벡터를 말합니다.
* **A = XΛX⁻¹ (비대칭 행렬의 고유값 분해)**:
    * 위와 비슷하지만, 대칭이 아닌 일반적인 '비대칭 행렬' A에 적용됩니다. 여기서도 $\text{Λ}$는 고유값, $\text{X}$는 고유 벡터들을 모아놓은 행렬입니다.
* **A = UΣVᵀ (특이값 분해, Singular Value Decomposition - SVD)**:
    * 이것은 선형대수학 이론의 **'가장 중요한 정점'**이라고 불릴 만큼 매우 중요합니다.
    * $\text{Σ (시그마)}$는 행렬의 중요한 정보를 담고 있는 **'특이값'**들을 포함하는 대각 행렬이며, $\text{U}$와 $\text{V}$는 특별한 직교 벡터들을 포함합니다.
    * SVD는 행렬을 아까 말했던 **랭크-1 행렬들의 합**으로 쪼개서, 아주 큰 데이터에서 핵심 정보만 뽑아내거나(저차원 근사), 노이즈를 제거하는 등 데이터 과학에서 광범위하게 활용됩니다.


이 강의 내용은 선형대수학의 핵심 개념들을 새로운 시각으로 보고, 이 개념들이 데이터 분석이나 문제 해결에 어떻게 응용되는지를 보여주기 위한 것입니다. 처음에는 어렵게 느껴질 수 있지만, 이런 분해 방법들이 마치 레고 블록처럼 복잡한 행렬을 더 작은 조각으로 나누어 이해하는 데 도움을 준다고 생각하면 훨씬 쉽게 다가올 거예요!

## 2. 선형대수학의 큰 그림: 데이터의 숨겨진 공간들을 파헤치다
- 출처:[Part 2: The Big Picture of Linear Algebra](https://www.youtube.com/watch?v=rwLOfdfc4dw&list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek&index=4)

이번에는 **'네 가지 기본 부분 공간'**이라는 선형대수학의 핵심 퍼즐 조각들을 맞춰보고, 복잡한 방정식을 푸는 중요한 방법인 **'소거법'**이 어떻게 행렬로 표현되는지 알아볼게요. 

이번 강의는 행렬이 가진 '숨겨진 공간들'을 찾아내고, 이 공간들이 서로 어떤 관계를 가지고 있는지 보여주는 것이 핵심입니다. 마치 어떤 건물에 여러 개의 방(공간)이 있는데, 각 방이 어떤 특징을 가지고 있고, 서로 어떻게 연결되어 있는지를 탐험하는 것과 비슷하다고 생각하시면 됩니다.

### 2.1 Ax = 0 과 영 공간 (Nullspace): 행렬이 '무시하는' 방향들

가장 먼저 다룰 방정식은 $\text{Ax = 0}$ 입니다.

* **$\text{Ax = 0}$의 의미**: 
  * 이건 "행렬 A의 모든 행(가로줄) 벡터와 어떤 벡터 $\text{x}$를 곱했더니 결과가 0이 되었다"는 뜻입니다. 수학적으로 '내적'이 0이라는 것은 두 벡터가 서로 **'수직(직교)'**이라는 의미예요.
    * **즉, $\text{Ax = 0}$은 '벡터 $\text{x}$가 A의 모든 행에 대해 수직인 경우를 찾아라!' 라는 질문과 같습니다.**
* **영 공간 N(A)**: 
  * 이렇게 $\text{Ax = 0}$을 만족하는 모든 $\text{x}$ 벡터들을 모아놓은 집합을 행렬 A의 **'영 공간(Nullspace)'**이라고 부릅니다. 이 공간은 행렬 A가 어떤 작용을 할 때, **'영향을 주지 않고 0으로 만들어 버리는'** 벡터들의 모임이에요.
* **영 공간의 크기 (차원)**: 
  * 영 공간에 속하는 독립적인 벡터의 개수는 $\text{n - r}$ 개입니다. 여기서 $\text{n}$은 A 행렬의 **열(세로줄) 개수**이고, $\text{r}$은 A 행렬의 **'랭크(rank)'**예요. 랭크는 그 행렬이 가진 '독립적인 정보의 양'을 나타냅니다.


### 2.2 전치 행렬 (A Transpose, Aᵀ) 과 Aᵀy = 0

* **전치 행렬 Aᵀ**: 
  * 어떤 행렬 A가 있을 때, A의 **가로줄을 세로줄로 바꾸고, 세로줄을 가로줄로 바꾼 행렬**을 '전치 행렬 Aᵀ'라고 부릅니다. 마치 표를 90도 회전시키는 것과 같아요.
* **Aᵀy = 0**: 
  * 이 방정식은 $\text{Aᵀ}$의 모든 행(원래 A의 열)과 벡터 $\text{y}$가 수직임을 의미합니다. 즉, $\text{y}$ 벡터들은 $\text{Aᵀ}$의 영 공간 $\text{N(Aᵀ)}$을 이룹니다. 이 $\text{N(Aᵀ)}$은 나중에 배울 A의 '열 공간'과 수직인 관계를 가집니다.


### 2.3 선형대수학의 네 가지 기본 부분 공간 (The Four Fundamental Subspaces)

길버트 스트랭 교수님은 선형대수학을 이해하는 '큰 그림'을 그리기 위해 이 네 가지 공간을 아는 것이 정말 중요하다고 강조합니다. 이 공간들은 두 쌍으로 짝을 이루며 서로 **'직교(수직)'**하는 관계를 가지고 있어요.

1.  **행 공간 (Row Space, C(Aᵀ))**:
    * **정의**: 행렬 A의 **독립적인 행(가로줄)들로 이루어진 공간**입니다. 쉽게 말해, A가 가진 '가로 방향 정보'의 핵심 덩어리입니다.
    * **크기 (차원)**: 행렬의 **랭크 $\text{r}$**과 같습니다.

2.  **영 공간 (Nullspace, N(A))**:
    * **정의**: $\text{Ax = 0}$을 만족하는 $\text{x}$ 벡터들의 공간입니다. (위에서 설명했죠?)
    * **관계**: 이 영 공간은 바로 **'행 공간'과 수직**입니다.
    * **크기 (차원)**: $\text{n - r}$ (A의 열 개수 - 랭크).

3.  **열 공간 (Column Space, C(A))**:
    * **정의**: 행렬 A의 **열(세로줄) 벡터들을 모든 방식으로 조합해서 만들 수 있는 공간**입니다. 쉽게 말해, A가 가진 '세로 방향 정보'가 표현할 수 있는 모든 결과물입니다.
    * **관계**: 이 열 공간은 **$\text{Aᵀ}$의 영 공간과 수직**입니다.
    * **크기 (차원)**: 행렬의 **랭크 $\text{r}$**과 같습니다.

4.  **A 전치 행렬의 영 공간 (Nullspace of A Transpose, N(Aᵀ))**:
    * **정의**: $\text{Aᵀy = 0}$을 만족하는 $\text{y}$ 벡터들의 공간입니다. (위에서 설명했죠?)
    * **관계**: 이 공간은 바로 **'열 공간'과 수직**입니다.
    * **크기 (차원)**: $\text{m - r}$ (A의 행 개수 - 랭크).

**요약하면**:
* **행 공간 (C(Aᵀ))**과 **영 공간 (N(A))**은 서로 수직 관계이며, 이 둘의 차원을 합치면 **총 열의 개수($\text{n}$)**가 됩니다.
* **열 공간 (C(A))**과 **$\text{Aᵀ}$의 영 공간 (N(Aᵀ))**은 서로 수직 관계이며, 이 둘의 차원을 합치면 **총 행의 개수($\text{m}$)**가 됩니다.

이 네 공간은 선형대수학의 기본 뼈대가 되며, 복잡한 행렬 문제를 해결하고 데이터를 이해하는 데 필수적인 개념입니다.


### 2.4 행렬 곱셈의 다른 방식: 열 곱하기 행 (Columns times Rows)

보통 행렬을 곱할 때는 앞 행렬의 '가로줄'과 뒤 행렬의 '세로줄'을 곱하는 방식을 쓰죠. 그런데 선형대수학에서는 **'열 곱하기 행'**이라는 다른 곱셈 방식도 중요하게 다룹니다.

* 이 방식은 행렬을 **'랭크-1 행렬'**들의 합으로 쪼갤 수 있게 해줍니다.
* **랭크-1 행렬**은 아주 간단한 행렬이에요. 모든 열이 첫 번째 열의 배수이고, 모든 행이 첫 번째 행의 배수인 형태를 가집니다. 마치 그림으로 치면 한 가지 색깔만으로 이루어진 그림 같아요.
* 이 랭크-1 행렬은 다른 복잡한 행렬을 만들 수 있는 **'가장 기본적인 블록'** 역할을 합니다. 큰 퍼즐을 작은 조각들로 나누어 생각하는 것과 같습니다.


### 2.5 A = LU 인수분해: 소거법으로 방정식 풀기

**'소거법(Elimination)'**은 우리가 중학교 때부터 배웠던 연립방정식 푸는 방법(한 문자를 없애는 방법)을 말합니다. 선형대수학에서는 이 소거법을 행렬의 곱셈으로 표현하는 'LU 인수분해'를 배웁니다.

* **목적**: 
  * 복잡한 선형 방정식 $\text{Ax=b}$ (예: $\text{2x + 3y = 7}$, $\text{4x + 7y = 15}$)을 체계적으로 푸는 가장 기본적인 방법을 행렬로 나타내는 것입니다.
* **과정**:
    1.  위 예시에서 $\text{x}$를 없애기 위해 첫 번째 식에 2를 곱해서 두 번째 식에서 빼면 $\text{y}$ 값을 구할 수 있습니다.
    2.  구한 $\text{y}$ 값을 다시 첫 번째 식에 대입하면 $\text{x}$ 값도 구할 수 있죠.
* **LU 인수분해**: 이렇게 방정식을 푸는 모든 단계를 행렬 곱셈으로 표현할 수 있습니다. 최종적으로 원래 행렬 $\text{A}$는 다음과 같이 두 행렬의 곱으로 나타낼 수 있습니다.
    * **$\text{A = L U}$**
    * **$\text{L}$ (Lower triangular matrix)**: 
      * '하삼각 행렬'이라고 부르며, 대각선 아래쪽에만 숫자가 있는 행렬입니다. 소거 과정에서 '빼주는 배수'들이 여기에 저장됩니다.
    * **$\text{U}$ (Upper triangular matrix)**: 
      * '상삼각 행렬'이라고 부르며, 대각선 위쪽에만 숫자가 있는 행렬입니다. 소거가 끝난 후의 '정리된' 행렬 형태입니다.
* **활용**: 
  * $\text{A=LU}$ 분해는 컴퓨터가 대규모 연립방정식을 풀 때 주로 사용하는 효율적인 방법입니다. 만약 소거 과정에서 행(가로줄)의 순서를 바꿔야 한다면, '순열 행렬(Permutation matrix) P'라는 것을 사용해서 $\text{PA=LU}$ 형태로 표현하기도 합니다.

## 3. 선형대수학: 데이터의 '수직' 관계를 탐험하다
- 출처: [Part 3: Orthogonal Vectors](https://www.youtube.com/watch?v=j8hEnyOiwhw&list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek&index=4)

이번 강의의 핵심은 **'직교(Orthogonal)'**라는 단어입니다. '직교'는 우리가 일상생활에서 쓰는 **'수직' 또는 '직각'**과 같은 의미예요. 선형대수학에서는 이 수직 관계가 행렬과 벡터를 이해하는 데 매우 중요하게 사용됩니다.

### 3.1 직교 벡터와 직교 행렬 (Orthogonal Vectors and Matrices)

* **직교의 의미**: 
  * 두 벡터가 서로 **'수직'**이라는 뜻입니다. 예를 들어, 우리 방의 바닥과 벽이 수직인 것처럼요. 수학적으로는 두 벡터를 곱했을 때 (내적했을 때) 0이 나오면 직교합니다.

* **직교 행렬 Q**:
    * **정의**: 
      * 이 행렬은 아주 특별한 행렬이에요. 이 행렬의 **모든 '세로줄 벡터'들이 서로 수직**이어야 하고, 각 세로줄 벡터의 **'길이(크기)'는 정확히 1**이어야 합니다. 마치 자기가 서있는 위치에서 1m짜리 자를 들고, 주변에 있는 모든 사람들과 90도로 서 있는 모습을 상상해보세요!
    * **수학적 표현**: 
      * 이런 직교 행렬 $\text{Q}$는 $\text{QᵀQ = I}$라는 아주 깔끔한 식으로 표현됩니다. 여기서 $\text{Qᵀ}$는 $\text{Q}$를 뒤집어 놓은 전치 행렬이고, $\text{I}$는 '단위 행렬'인데, 마치 숫자 1처럼 곱해도 자기 자신이 되는 특별한 행렬입니다. 만약 $\text{Q}$가 네모난 행렬(정방 행렬)이라면, $\text{Qᵀ}$는 $\text{Q}$의 역행렬과 똑같아요 ($\text{Qᵀ = Q⁻¹}$).
    * **예시**: 
      * 우리가 아는 $\text{x, y, z}$ 축은 서로 완벽하게 수직이죠? 이 축들을 나타내는 벡터들이 바로 직교 벡터의 좋은 예입니다. 또, 물체를 돌리는 **'회전 행렬'**도 직교 행렬의 좋은 예시예요.
    * **장점**: 
      * 직교 행렬은 **'계산에 가장 좋은 행렬'**이라고 불립니다. 왜냐하면 어떤 벡터에 직교 행렬을 곱해도 벡터의 **'길이'가 변하지 않기 때문**이에요. 이 특성 덕분에 계산 중에 숫자가 너무 커지거나(폭발) 너무 작아져서(0으로 사라짐) 생기는 오차를 막아줍니다. 여러 개의 직교 행렬을 계속 곱해도 여전히 직교 행렬이 됩니다.


### 3.2 A = QR 인수분해 및 그램-슈미트 과정

직교 행렬이 계산에 이렇게나 좋으니, 일반적인 행렬 $\text{A}$를 직교 행렬 $\text{Q}$와 다른 행렬 $\text{R}$의 곱으로 바꾸고 싶겠죠? 그게 바로 $\text{A = QR}$ 인수분해입니다.

* **목적**: 
  * $\text{A}$의 열 벡터들을 **서로 수직인(직교하는) 열 벡터들**로 바꿔서 $\text{Q}$ 행렬을 만드는 것이 목표입니다.
* **그램-슈미트 과정**: 
  * 독립적인 열들을 가진 행렬 $\text{A}$를 '직교하는' 열들을 가진 $\text{Q}$ 행렬로 바꿔주는 특별한 방법이 있는데, 이를 **'그램-슈미트 과정'**이라고 합니다. 마치 삐뚤빼뚤한 나무 기둥들을 깔끔하게 수직으로 세우는 과정과 비슷해요.
* **인수분해**: 
  * 이 과정을 거치면 $\text{A}$는 **직교 행렬 $\text{Q}$**와 **삼각 행렬 $\text{R}$**의 곱인 $\text{A = QR}$ 형태로 분해됩니다. (삼각 행렬 $\text{R}$은 숫자가 삼각형 모양으로 채워진 행렬을 말합니다.)
* **중요성**: 
  * $\text{A=QR}$ 분해는 선형대수학, 특히 **컴퓨터를 이용한 계산 선형대수학**에서 매우 중요한 기본 단계다.


### 3.3 최소 제곱법 (Least Squares): A = QR의 주요 활용

* **문제**: 
  * 실제 데이터를 다루다 보면, 우리가 풀려는 방정식 $\text{Ax = b}$이 **방정식 개수(m)보다 미지수의 개수(n)가 더 적은 경우**가 많아요 ($\text{m > n}$). 예를 들어, 데이터는 10개인데 찾으려는 미지수는 3개인 경우죠. 이런 경우엔 보통 정확한 해를 찾을 수 없습니다. (데이터가 너무 많아서 모든 데이터를 정확히 만족하는 $\text{x}$가 존재하지 않아요.)
* **목표**: 
  * 정확한 해는 없지만, 그래도 $\text{b}$에 **가장 가까운 해 $\text{x̂}$**를 찾아서, 오차 $\text{e = b - Ax}$의 크기를 **가장 작게 만드는 것**이 목표입니다. 예를 들어, 여러 점들을 가장 잘 대표하는 '최적의 선'을 찾는 문제와 같아요.
* **정규 방정식**: 
  * 이런 문제를 풀기 위해 $\text{AᵀAx̂ = Aᵀb}$라는 특별한 방정식을 사용합니다. 이 방정식의 해 $\text{x̂}$가 바로 우리가 찾는 '최소 제곱 해'입니다.
* **$\text{AᵀA}$ 행렬의 특징**: 
  * 이 $\text{AᵀA}$ 행렬은 항상 **정방형(네모난 형태)**이고, **대칭(좌우가 같은)**이며, 계산에 유리한 좋은 성질들을 가지고 있습니다.
* **기하학적 해석**: 
  * 최소 제곱법은 $\text{b}$ 벡터를 A의 **'열 공간'**이라는 곳에 **'수직으로 투영'**해서 가장 가까운 점 $\text{p}$를 찾는 과정과 같습니다. $\text{b}$와 $\text{p}$ 사이의 오차 $\text{e}$는 이 열 공간과 수직이 됩니다.
* **QR 인수분해의 역할**: 
  * $\text{A=QR}$ 분해를 최소 제곱법에 적용하면 $\text{Rx̂ = Qᵀb}$라는 더 간단한 형태로 방정식을 바꿔서, $\text{x̂}$를 더 쉽게 계산할 수 있게 해줍니다.


### 3.4 대칭 행렬과 고유값/고유벡터 (Symmetric Matrices and Eigenvalues/Eigenvectors)

* **대칭 행렬 S**: 
  * 자기 자신을 뒤집어도 똑같은 행렬($\text{S = Sᵀ}$)을 말합니다.
* **중요한 특성**: 
  * 대칭 행렬은 항상 **'실수 고유값'**을 가지고, 그 고유값에 해당하는 **'고유벡터'들은 항상 서로 수직(직교)합니다!** 이 직교하는 특성 때문에 대칭 행렬 $\text{S}$는 $\text{S = QΛQᵀ}$처럼 아주 깔끔하게 분해될 수 있습니다. (여기서 $\text{Q}$는 고유벡터들로 이루어진 직교 행렬이고, $\text{Λ}$는 고유값들을 대각선에 가진 행렬입니다.)


### 3.5 특이값 분해 (Singular Value Decomposition - SVD)

SVD는 선형대수학의 **'가장 최고점'**이라고 불릴 만큼 중요한 개념입니다. 어떤 행렬 $\text{A}$라도 세 개의 특별한 행렬 $\text{U}$, $\text{Σ}$, $\text{V}$의 곱인 $\text{A = UΣVᵀ}$ 형태로 분해하는 것입니다.

* **구성 요소**:
    * $\text{U}$: '좌 특이 벡터'라는 직교하는 세로줄 벡터들로 이루어진 행렬입니다.
    * $\text{Σ (시그마)}$: **'특이값'**이라는 중요한 값들을 대각선에 가진 행렬입니다. 이 특이값들은 항상 양수이고, 큰 값부터 순서대로 나열됩니다. 이 값들이 바로 행렬 $\text{A}$가 가진 **'정보의 중요도'**를 나타냅니다.
    * $\text{V}$: '우 특이 벡터'라는 직교하는 세로줄 벡터들로 이루어진 행렬입니다.
* **관계**: 
  * $\text{V}$의 열 벡터들은 $\text{AᵀA}$의 고유벡터들이고, $\text{U}$의 열 벡터들은 $\text{AAᵀ}$의 고유벡터들입니다. 그리고 특이값 $\text{σ}$는 $\text{AᵀA}$ 또는 $\text{AAᵀ}$의 고유값 $\text{λ}$의 양의 제곱근($\text{σ = √λ}$)입니다.
* **기하학적 해석**: 
  * SVD는 행렬 $\text{A}$가 어떤 '원'을 '타원'으로 어떻게 바꾸는지를 보여줍니다. $\text{V}$는 원의 초기 회전, $\text{Σ}$는 축을 따라 늘리거나 줄이는 변형, $\text{U}$는 최종적인 회전을 의미합니다.
* **랭크 1 행렬의 합**: 
  * SVD를 통해 $\text{A}$를 아까 배웠던 가장 간단한 '랭크 1 행렬'들의 합으로 표현할 수 있습니다.
* **저차원 근사 (Low-rank Approximation)**: 
  * 이것이 SVD의 가장 강력한 응용입니다! 가장 큰 특이값 몇 개와 그에 해당하는 벡터들만 사용해서 원래 행렬 $\text{A}$를 **'가장 비슷하게' 표현하는 더 작은 행렬**을 만들 수 있습니다. 예를 들어, 손상된 사진에서 노이즈를 제거하거나, 대용량 데이터를 압축하거나, 넷플릭스 같은 서비스에서 영화를 추천해주는 시스템 등에 핵심적으로 사용됩니다. 특이값이 클수록 중요한 정보라고 생각하면 돼요.


### 3.6 확률적 수치 선형대수학 (Randomized Numerical Linear Algebra)

요즘처럼 데이터가 너무너무 커질 때, 일반적인 계산 방법으로는 시간이 너무 오래 걸릴 수 있습니다. 이럴 때 **'확률화(randomization)'**라는 새로운 기술을 사용해서 행렬 계산을 훨씬 더 효율적으로 할 수 있게 되었어요. 예를 들어, 큰 행렬 두 개를 곱할 때, 전체를 다 계산하는 대신 일부만 '샘플링'해서 빠르고 정확하게 결과를 얻는 방식입니다.


이번 강의에서는 '직교'라는 개념이 행렬의 중요한 분해 방법들($\text{A=QR}$과 $\text{A=UΣVᵀ}$)과 어떻게 연결되고, 이 분해들이 데이터의 특징을 파악하거나(SVD) 최적의 해를 찾는(최소 제곱법) 데 어떻게 활용되는지 배웠습니다. 

## 4. 선형대수학: 행렬의 '심장'을 들여다보다 - 고유값과 고유벡터
- 출처: [Part 4: Eigenvalues and Eigenvectors](https://www.youtube.com/watch?v=GyC3gl6weYo&list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek&index=5)

우리는 지금까지 행렬을 계산하고 분해하는 여러 방법을 배웠습니다. 이제는 행렬이 어떤 방식으로 움직이고 변화하는지를 더 깊이 이해하는 방법을 알아볼 거예요. 마치 복잡한 기계의 움직임을 분석할 때, 그 기계의 핵심 부품과 움직이는 방향, 그리고 얼마나 빠르게 움직이는지를 파악하는 것과 비슷합니다.

### 4.1 고유값과 고유벡터 (Eigenvalues and Eigenvectors)

* **개념의 중요성**: 
  * 고유값과 고유벡터는 선형대수학에서 행렬을 **가장 깊이 있게 이해**할 수 있게 해주는 열쇠입니다. 행렬이 가진 **고유한 특성**을 파악하는 새로운 방법이라고 생각할 수 있어요.

* **정의 방정식**: 
  * 어떤 행렬 $\text{S}$에 벡터 $\text{x}$를 곱했을 때, $\text{Sx = λx}$ 라는 특별한 관계가 성립하는 경우가 있다.
    * 이때의 특별한 벡터 $\text{x}$를 **'고유벡터(eigenvector)'**라고 부릅니다.
    * $\text{λ (람다)}$라는 숫자를 **'고유값(eigenvalue)'**이라고 합니다.

* **기하학적 의미 (쉽게 설명하면)**:
    * **고유벡터 $\text{x}$**: 
      * 행렬 $\text{S}$가 벡터 $\text{x}$를 변화시킬 때, **$\text{x}$의 '방향'은 바뀌지 않고 그대로 유지되는** 아주 특별한 벡터입니다. 마치 어떤 물체에 힘을 가했는데, 그 물체가 회전하거나 다른 방향으로 가지 않고 원래 방향으로만 쭉 밀리거나 당겨지는 것과 같아요.
    * **고유값 $\text{λ}$**: 
      * 이 고유벡터가 행렬 $\text{S}$에 의해 **얼마나 늘어나거나 줄어드는지(크기가 변하는 정도)**를 나타내는 숫자입니다. 고유값이 2라면 2배로 늘어나고, 0.5라면 절반으로 줄어들고, -1이라면 방향이 완전히 뒤집히는 거죠.

* **계산의 단순화**:
  * 행렬 연산은 보통 행과 열 사이의 모든 관계를 고려해야 해서 복잡합니다. 하지만 고유벡터의 방향을 따라가면, 복잡한 행렬 문제가 마치 **'한 방향으로만 움직이는' 1차원 문제**처럼 단순해져서 계산이 훨씬 쉬워집니다.


### 4.2 대칭 행렬 (Symmetric Matrices): 선형대수학의 '왕'

* **정의**: 
  * 대칭 행렬 $\text{S}$는 가로 세로를 뒤집어도 자기 자신과 똑같은 행렬입니다 ($\text{S = Sᵀ}$). 거울에 비춘 것처럼 주대각선(왼쪽 위에서 오른쪽 아래로 가는 대각선)을 기준으로 대칭인 행렬이에요.
* **선형대수학의 '왕'**: 
  * 대칭 행렬은 그 특성이 너무나 '아름답고' 중요해서 이렇게 불립니다.

* **가장 중요한 특성: 직교 고유벡터**:
    * 대칭 행렬 $\text{S}$의 고유벡터들은 항상 **서로 수직(직교)**입니다! ($\text{xᵀy = 0}$, 즉 두 벡터의 내적이 0).
    * **스펙트럴 정리 (Spectral Theorem)**: 
        * 이 놀라운 직교성 덕분에 대칭 행렬 $\text{S}$는 $\text{S = QΛQᵀ}$ 라는 형태로 아주 깔끔하게 분해될 수 있습니다.
        * $\text{Q}$는 대칭 행렬의 **직교하는 고유벡터들을 세로줄로 모아놓은 직교 행렬**입니다. (모든 열 벡터가 서로 수직이고 길이가 1인 행렬)
        * $\text{Λ (람다)}$는 고유값들을 대각선에만 가진 **대각 행렬**입니다.
        * $\text{Qᵀ}$는 $\text{Q}$의 역행렬과 똑같습니다 ($\text{Qᵀ = Q⁻¹}$).
    * 이 '스펙트럴 정리'는 선형대수학뿐만 아니라 수학 전체에서도 **가장 중요한 정리 중 하나**로 꼽힙니다. 복잡한 대칭 행렬을 고유값과 직교하는 고유벡터로 단순하게 표현할 수 있게 해주거든요.


### 4.3 비대칭 행렬 (Non-Symmetric Matrices)

* **고유벡터의 비직교성**: 
  * 행렬 $\text{A}$가 대칭이 아닌 경우에는 고유벡터들이 더 이상 서로 수직이지 않습니다.
* **분해**: 
  * 그럼에도 불구하고, 만약 $\text{A}$가 '네모난 행렬'(정방 행렬)이고 $\text{n}$개의 독립적인 고유벡터를 가진다면, $\text{A = XΛX⁻¹}$ 형태로 분해될 수 있습니다. (여기서 $\text{X}$는 고유벡터들을 모아놓은 행렬, $\text{Λ}$는 고유값들을 가진 대각 행렬입니다.)
* **행렬의 거듭제곱**: 
  * 고유벡터 $\text{x}$는 행렬 $\text{A}$를 여러 번 곱해도(예: $\text{A², A³}$, ...) **방향이 변하지 않습니다.** 오직 고유값 $\text{λ}$만 해당 횟수만큼 거듭제곱됩니다 ($\text{Aᵏx = λᵏx}$).
    * 만약 모든 고유값 $\text{λ}$의 절댓값이 1보다 작으면($\text{\|λ\|<1}$), 행렬 $\text{A}$를 계속 곱할수록($\text{Aⁿ}$) 그 결과는 점점 0에 가까워집니다. 이는 시스템의 안정성 등을 분석하는 데 중요한 '깊은' 특성을 보여줍니다.


### 4.4 $\text{AᵀA}$ 행렬의 특성 및 최소 제곱법

* **$\text{AᵀA}$ 행렬의 생성**: 
  * 우리가 이전에 배웠던 직사각 행렬 $\text{A}$ (예: 데이터가 10줄, 특징이 3가지인 행렬)가 있을 때, $\text{A}$의 전치 행렬 $\text{Aᵀ}$와 $\text{A}$를 곱하면 ($\text{AᵀA}$) 항상 '네모난 행렬'이 됩니다.
* **핵심 특성**: $\text{AᵀA}$ 행렬은 놀랍게도 항상 다음과 같은 아주 좋은 특성을 가집니다.
    1.  **정방 행렬 (Square Matrix)**: 
    * 네모난 형태입니다.
    2.  **대칭 행렬 (Symmetric Matrix)**: 
    * 스스로 뒤집어도 똑같습니다. ($\text{(AᵀA)ᵀ = Aᵀ(Aᵀ)ᵀ = AᵀA}$). 이 대칭성 덕분에 $\text{AᵀA}$는 **항상 서로 수직인 고유벡터**를 가집니다.
    3.  **음이 아닌 준정부호 (Non-negative definite)**: 
    * 이 복잡한 이름은 쉽게 말해, $\text{AᵀA}$ 행렬의 모든 고유값이 **항상 0보다 크거나 같다($\text{λ ≥ 0}$)**는 뜻입니다. 이것은 어떤 벡터 $\text{x}$에 대해 $\text{xᵀ(AᵀA)x}$ 값이 항상 0 이상이라는 것을 의미하며, 이 값은 $(\text{Ax})ᵀ(\text{Ax})$ 또는 벡터 $\text{Ax}$의 길이의 제곱 $\text{\|\|Ax\|\|²}$과 같습니다. 벡터의 길이는 항상 0 이상이므로, 그 제곱도 0 이상이 되는 것이 당연하겠죠?
* **최소 제곱법과의 연관성**: 
  * $\text{AᵀA}$ 행렬은 지난 시간에 배웠던 **'최소 제곱법'**에서 아주 중요한 역할을 합니다. $\text{Ax=b}$ 같은 방정식이 정확한 해를 가지지 못할 때, 가장 비슷한 해를 찾기 위한 '정규 방정식' $\text{AᵀAx̂ = Aᵀb}$ 에 바로 이 $\text{AᵀA}$가 등장합니다. $\text{AᵀA}$의 좋은 특성들 덕분에 이 방정식이 항상 해를 가질 수 있게 됩니다.


### 4.5 특이값 분해 (Singular Value Decomposition - SVD)로의 전환

* **비정방 행렬의 문제**: 
  * 실제 데이터는 대부분 '네모나지 않은'(직사각 행렬) 형태를 가집니다. 이런 행렬에는 고유값과 고유벡터의 개념을 바로 적용하기 어렵습니다.
* **특이값의 역할**: 
  * 이때 **'특이값(Singular Values)'**이라는 개념이 고유값의 역할을 대신합니다. 특이값은 $\text{AᵀA}$와 $\text{AAᵀ}$이라는 특별한 대칭 행렬들의 고유값을 계산함으로써 얻을 수 있습니다.
* **SVD로 이어지다**: 
  * 이 특이값들을 통해 우리는 어떤 직사각 행렬 $\text{A}$라도 $\text{A = UΣVᵀ}$ 라는 형태로 분해할 수 있는 **'특이값 분해(SVD)'**로 나아가게 됩니다.
    * $\text{U}$와 $\text{V}$는 각각 A의 왼쪽과 오른쪽에서 작용하는 직교 행렬이고,
    * $\text{Σ}$는 '특이값'들을 대각선에 가진 행렬입니다.
* **"데이터를 다루는 데 큰 전환점"**: 
  * SVD는 복잡한 데이터 분석, 압축, 노이즈 제거, 추천 시스템 등 현대 데이터 과학에서 **없어서는 안 될 핵심 도구**입니다. 이 강의를 통해 SVD의 중요성을 배우는 것이 선형대수학 과정에서 가장 중요한 부분 중 하나라고 강조하는 이유가 바로 여기에 있습니다.


이번 강의는 행렬의 가장 본질적인 특성을 나타내는 고유값과 고유벡터를 이해하는 것이 핵심입니다. 특히 대칭 행렬의 아름다운 직교 고유벡터 특성과, 이 개념이 어떻게 최소 제곱법과 같은 응용으로 이어지는지, 그리고 궁극적으로 직사각 행렬을 다루는 SVD로 확장되는지를 배웠습니다.

## 5. 선형대수학: 어떤 데이터라도 쪼개고 분석하는 SVD의 마법
- 출처: [Part 5: Singular Values and Singular Vectors](https://www.youtube.com/watch?v=IHO7_n7Y09s&list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek&index=6)

이번에는 선형대수학의 '정점'이자 데이터 과학에서 정말 많이 쓰이는 **'특이값 분해(SVD)'**에 대해 알아볼 거예요. 지금까지 배운 고유값/고유벡터는 네모난 행렬(정방 행렬)에만 쓸 수 있었지만, SVD는 **세상 모든 행렬**에 적용할 수 있는 마법 같은 도구랍니다!

우리가 다루는 대부분의 데이터는 항상 네모난(정방) 형태가 아니에요. 사진 데이터는 가로 픽셀 수와 세로 픽셀 수가 다를 수 있고, 설문조사 데이터도 질문 수와 응답자 수가 다를 수 있죠. 이런 '직사각 행렬'을 분석할 때 고유값/고유벡터는 바로 쓸 수 없다는 한계가 있었어요.

이때 등장하는 것이 바로 **특이값 분해(SVD)**입니다. SVD는 어떤 모양의 행렬이든, 심지어 '못생긴' 직사각 행렬까지도 멋지게 분석하고 분해할 수 있게 해주는 아주 강력한 도구입니다.

### 5.1 특이값과 특이벡터: 새로운 '심장'과 '방향'

* **고유값/고유벡터의 한계 극복**: 
  * 고유값과 고유벡터가 정방 행렬만 분석할 수 있었다면, **특이값(singular values)**과 **특이벡터(singular vectors)**는 이 한계를 넘어서서 **모든 행렬**에 적용할 수 있는 '완벽한 대체재'입니다.

* **정의 방정식**: 행렬 $\text{A}$에 어떤 벡터 $\text{v}$를 곱했을 때 $\text{Av = σu}$라는 특별한 관계가 성립하는 경우를 찾습니다. (이전의 $\text{Sx = λx}$와 비슷해 보이죠?)
    * $\text{u}$: 
      * $\text{A}$가 $\text{v}$를 변환시킨 후의 방향을 나타내는 **'좌특이벡터(left singular vector)'**입니다.
    * $\text{v}$: 
      * $\text{A}$가 변환하기 **전의 방향**을 나타내는 **'우특이벡터(right singular vector)'**입니다.
    * $\text{σ (시그마)}$: 
      * 벡터가 그 방향으로 얼마나 **늘어나거나 줄어드는지(크기가 변하는 정도)**를 나타내는 **'특이값(singular value)'**입니다. 특이값은 항상 양수이고, 중요한 순서대로 큰 값부터 작은 값으로 정렬됩니다. 행렬 A가 가진 **정보의 '중요도'**를 숫자로 나타낸다고 생각하면 됩니다.


### 5.2 특이값 분해 (SVD)의 구성: A = UΣVᵀ

모든 $\text{m x n}$ 크기(가로 $\text{m}$줄, 세로 $\text{n}$줄)의 행렬 $\text{A}$는 항상 세 개의 특별한 행렬의 곱으로 쪼개질 수 있습니다: $\text{A = UΣVᵀ}$.

* **$\text{U}$**: 
  * **좌특이벡터들($\text{uᵢ}$)**을 세로줄로 가진 **직교 행렬**입니다. ($\text{UᵀU = I}$). $\text{U}$는 행렬 A가 벡터를 변환시킨 후의 최종 '회전'을 담당합니다.
* **$\text{Σ}$ (시그마)**: 
  * **특이값들($\text{σᵢ}$)**을 대각선에만 가진 **대각 행렬**입니다. 이 행렬이 벡터의 **'길이'를 늘리거나 줄이는 역할**을 합니다.
* **$\text{Vᵀ}$**: 
  * **우특이벡터들($\text{vᵢ}$)**을 가로줄로 가진 **직교 행렬 $\text{V}$의 전치 행렬**입니다. ($\text{VᵀV = I}$). $\text{Vᵀ}$는 벡터를 변환하기 전의 '초기 회전'을 담당합니다.

* **직교성의 중요성**: 
  * 고유벡터는 대칭 행렬에서만 서로 직교했지만, **특이벡터($\text{u}$와 $\text{v}$)는 어떤 행렬 $\text{A}$라도 항상 서로 직교하도록 만들 수 있습니다.** 이게 SVD의 핵심적인 강점 중 하나입니다.


### 5.3 SVD의 기하학적 의미: '회전-확장-회전'

SVD는 행렬 $\text{A}$가 벡터를 변환시키는 과정을 세 단계의 간단한 기하학적 움직임으로 설명합니다:

1.  **첫 번째 회전 ($\text{Vᵀ}$)**: 
  * 먼저 원래의 벡터를 $\text{Vᵀ}$가 **회전시킵니다.** 이때 벡터의 길이는 변하지 않아요.
2.  **확장 ($\text{Σ}$)**: 
  * 회전된 벡터는 $\text{Σ}$에 의해 각 축 방향으로 **특이값 $\text{σᵢ}$만큼 늘어나거나 줄어듭니다.** 이 과정에서 동그란 원이 찌그러져서 타원 모양으로 변합니다.
3.  **두 번째 회전 ($\text{U}$)**: 
  * 마지막으로 $\text{U}$가 이 타원을 다시 **회전시킵니다.** 이때도 길이는 변하지 않고 방향만 바뀝니다.

요약하면, SVD는 어떤 복잡한 행렬 변환이라도 **'회전 → 늘리고 줄이기(확장) → 다시 회전'**이라는 세 가지 간단한 동작의 조합으로 완벽하게 설명할 수 있다는 것을 보여줍니다.


### 5.4 특이벡터를 찾는 방법: $\text{AᵀA}$ 행렬의 활용

특이벡터 $\text{u}$와 $\text{v}$는 어떻게 찾을까요? 놀랍게도 지난 강의에서 배웠던 $\text{AᵀA}$ 행렬을 활용합니다.

* **우특이벡터 ($\text{V}$) 찾기**: 
  * 행렬 $\text{AᵀA}$의 **고유벡터들**이 바로 행렬 $\text{A}$의 **우특이벡터 $\text{v}$들**입니다.
    * $\text{AᵀA}$는 항상 '네모나고', '대칭'이며, 모든 고유값이 0보다 크거나 같은 '좋은' 행렬이기 때문에, 그 고유벡터들은 항상 직교하고 고유값은 0 이상입니다.
    * $\text{AᵀA}$의 고유값 $\text{λ}$는 $\text{A}$의 **특이값 $\text{σ}$의 제곱($\text{σ²}$)**과 같습니다. 즉, $\text{σ = √λ}$ 입니다.

* **좌특이벡터 ($\text{U}$) 찾기**: 
  * 우특이벡터 $\text{v}$와 특이값 $\text{σ}$를 알면 $\text{u = Av / σ}$라는 간단한 식을 통해 좌특이벡터 $\text{u}$도 찾을 수 있습니다. 이 $\text{u}$들도 서로 직교합니다.


### 5.5 SVD의 중요성 및 응용: 데이터 과학의 핵심 도구

* **최고의 행렬 분해**: 
  * SVD는 "모든 행렬 분해 중 최고"라고 불릴 정도로 그 가치가 높이 평가되며, 특히 지난 20~30년간 데이터 과학의 발전과 함께 그 중요성이 더욱 커졌습니다.

* **데이터 과학에서의 활용**:
    * **중요한 부분 추출**: 
      * SVD는 거대한 데이터 행렬에서 **"무엇이 가장 중요한 정보인가"**를 파악하는 데 결정적인 역할을 합니다. 특이값 $\text{σ₁ ≥ σ₂ ≥ ...}$의 크기는 데이터 내의 정보들이 **얼마나 중요한지 순서대로** 보여줍니다.
    * **저랭크 근사 (Low-Rank Approximation)**: 
      * 거대한 데이터 행렬에서 가장 큰 **몇 개의 특이값과 그에 해당하는 특이벡터들만 사용하여** 원래 행렬을 가장 잘 비슷하게(근사하게) 표현하는 더 작은 행렬을 만들 수 있습니다. 예를 들어, 잡음이 많은 사진에서 노이즈를 제거하고 핵심만 남기거나, 넷플릭스 같은 서비스에서 방대한 사용자 데이터를 기반으로 개인에게 영화를 추천해주는 시스템 등에 SVD가 핵심적으로 사용됩니다.
    * **다양한 실제 응용**: 
      * 금융 시장에서 채권 가격을 예측하거나, 고대 인류의 역사를 연구하는 등 복잡하고 방대한 데이터를 처리할 때, SVD는 숨겨진 패턴과 중요한 특징을 찾아내는 데 필수적인 도구입니다.

* **무작위 수치 선형대수학**: 
  * 요즘처럼 데이터가 너무 커서 SVD를 직접 계산하기 어려울 때는, 데이터를 **무작위로 일부만 뽑아내서(샘플링)** SVD를 근사적으로 계산하는 새로운 기법도 사용됩니다. 이는 데이터 자체에 어떤 규칙적인 구조가 숨어있기 때문에 가능한 방법입니다.


이번 강의는 선형대수학의 가장 강력하고 유용한 도구인 **특이값 분해(SVD)**를 배웠습니다. SVD는 모든 종류의 행렬을 분석하고, 데이터의 핵심적인 부분을 찾아내며, 나아가 데이터를 압축하고 노이즈를 제거하는 등 현대 데이터 과학의 많은 문제들을 해결하는 데 필수적인 열쇠입니다.
